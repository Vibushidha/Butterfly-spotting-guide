import streamlit as st
import random
import pydeck as pdk
import pandas as pd
import re
import speech_recognition as sr
import os
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import av # You might need this package too. Add 'av' to requirements.txt

# -----------------------------
# üåø App Config
# -----------------------------
st.set_page_config(page_title="Butterfly Spotting Guide ü¶ã", page_icon="ü¶ã", layout="centered")

st.markdown(
    "<h1 style='text-align:center; color:#2E8B57;'>ü¶ã Butterfly Spotting Guide üåø</h1>",
    unsafe_allow_html=True
)
st.write("Describe a butterfly, upload an image, or use your voice to identify it!")

# -----------------------------
# ü¶ã Butterfly Data
# -----------------------------
# Use os.path.join to ensure the path is correctly constructed relative to the app.py file
# os.path.dirname(__file__) gets the directory where app.py is located.
IMAGE_DIR = os.path.join(os.path.dirname(__file__), "data", "sample_images")

butterflies = {
    "Monarch": os.path.join(IMAGE_DIR, "monarch.jpg"),
    "Swallowtail": os.path.join(IMAGE_DIR, "swallowtail.jpg"),
    "Blue Morpho": os.path.join(IMAGE_DIR, "blue_morpho.jpg"),
    "Painted Lady": os.path.join(IMAGE_DIR, "painted_lady.jpg"),
    "Common Jezebel": os.path.join(IMAGE_DIR, "common_jezebel.jpg"),
    "Peacock": os.path.join(IMAGE_DIR, "peacock.jpg"),
    "Red Admiral": os.path.join(IMAGE_DIR, "red_admiral.jpg"),
}
facts = {
    "Monarch": "üß≠ Monarchs migrate over 3,000 miles from North America to Mexico!",
    "Swallowtail": "üåº Swallowtails have tails that resemble a bird‚Äôs feathers!",
    "Blue Morpho": "‚ú® Their bright blue shimmer comes from reflected light!",
    "Painted Lady": "üåç Found on every continent except Antarctica!",
    "Common Jezebel": "üé® Famous for their red, yellow, and white wing patterns.",
    "Peacock": "üëÅÔ∏è Their wings have big eye-like spots to scare predators!",
    "Red Admiral": "ü¶ã Bold and territorial ‚Äî they love basking in the sun!",
}

# -----------------------------
# üåç Migration Data
# -----------------------------
migration_data = {
    "Monarch": [
        {"month": "Jan", "lat": 19.4, "lon": -99.1, "place": "Mexico", "reason": "Overwintering in warm forests", "fact": "They love the sunshine here ‚òÄÔ∏è"},
        {"month": "Mar", "lat": 30.0, "lon": -90.0, "place": "Southern US", "reason": "Starting their northward journey", "fact": "They travel up to 100 miles a day! üí™"},
        {"month": "Jun", "lat": 44.9, "lon": -93.0, "place": "Midwest USA", "reason": "Breeding season", "fact": "They rest on milkweed fields üåø"},
        {"month": "Sep", "lat": 35.7, "lon": -78.6, "place": "Southeast USA", "reason": "Heading south again", "fact": "Time to migrate home! üß≠"},
    ],
    "Swallowtail": [
        {"month": "Jan", "lat": 20.0, "lon": 78.0, "place": "India", "reason": "Staying warm", "fact": "They love tropical nectar! üå∫"},
        {"month": "Apr", "lat": 30.0, "lon": 100.0, "place": "China", "reason": "Breeding time", "fact": "They dance in spring breezes üíÉ"},
        {"month": "Jul", "lat": 45.0, "lon": 10.0, "place": "Europe", "reason": "Exploring meadows", "fact": "They flutter gracefully in fields üåæ"},
        {"month": "Oct", "lat": 25.0, "lon": 77.0, "place": "Back to India", "reason": "Return migration", "fact": "Home sweet home üè°"},
    ],
    "Blue Morpho": [
        {"month": "Jan", "lat": -3.0, "lon": -60.0, "place": "Amazon", "reason": "Lives in rainforests", "fact": "Their wings shine like gems üíé"},
        {"month": "Jun", "lat": -5.0, "lon": -75.0, "place": "Peru", "reason": "Following humidity patterns", "fact": "They love misty mornings üå´Ô∏è"},
    ],
    "Painted Lady": [
        {"month": "Feb", "lat": 0.0, "lon": 35.0, "place": "Kenya", "reason": "Winter rest", "fact": "They chill where flowers bloom üå∏"},
        {"month": "May", "lat": 45.0, "lon": 2.0, "place": "France", "reason": "Migrating north", "fact": "They fly thousands of miles üöÄ"},
        {"month": "Sep", "lat": 30.0, "lon": 31.0, "place": "Egypt", "reason": "Heading back south", "fact": "They love warm deserts ‚òÄÔ∏è"},
    ],
    "Common Jezebel": [
        {"month": "Mar", "lat": 13.0, "lon": 80.0, "place": "South India", "reason": "Breeding season", "fact": "They love blooming trees üå≥"},
        {"month": "Jun", "lat": 28.0, "lon": 77.0, "place": "North India", "reason": "Monsoon migration", "fact": "They follow the rains ‚òî"},
        {"month": "Oct", "lat": 19.0, "lon": 72.0, "place": "West India", "reason": "Settling before winter", "fact": "They enjoy coastal breezes üåä"},
    ],
    "Peacock": [
        {"month": "Apr", "lat": 51.5, "lon": -0.1, "place": "UK", "reason": "Emerging from hibernation", "fact": "They love sunny walls ‚òÄÔ∏è"},
        {"month": "Jul", "lat": 48.8, "lon": 2.3, "place": "France", "reason": "Feeding on flowers", "fact": "They adore lavender fields üíú"},
        {"month": "Sep", "lat": 52.5, "lon": 13.4, "place": "Germany", "reason": "Preparing for winter", "fact": "They start hibernating soon ‚ùÑÔ∏è"},
    ],
    "Red Admiral": [
        {"month": "Jan", "lat": 20.0, "lon": -15.0, "place": "West Africa", "reason": "Winter rest", "fact": "They soak in the warmth üåû"},
        {"month": "May", "lat": 48.0, "lon": 11.0, "place": "Germany", "reason": "Breeding season", "fact": "They love stinging nettles üåø"},
        {"month": "Oct", "lat": 41.9, "lon": 12.5, "place": "Italy", "reason": "Heading south again", "fact": "Ciao bella! üáÆüáπ"},
    ],
}
# Add this class definition somewhere before your main interface code
class AudioRecorder(AudioProcessorBase):
    def __init__(self):
        self.audio_chunks = []
        self.is_recording = False

    # This method is called repeatedly with audio frames
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        if self.is_recording:
            # We capture the raw audio data from the frame
            self.audio_chunks.append(frame.to_ndarray())
        return frame

# -----------------------------
# üéôÔ∏è Voice Recognition
# -----------------------------

# Initialize the flag BEFORE the try block to avoid NameError
VOICE_AVAILABLE = False 

try:
    import speech_recognition as sr
    VOICE_AVAILABLE = True # Set to True only if the import is successful
    
    # Define the function only if the import succeeds
    def listen_to_voice():
        r = sr.Recognizer()
        try:
            with sr.Microphone() as source:
                st.info("üé§ Listening... describe the butterfly clearly (you have 5 seconds)...")
                r.adjust_for_ambient_noise(source, duration=1)
                # The problematic line that raises PyAudio errors during runtime:
                audio = r.listen(source, phrase_time_limit=5) 
                try:
                    text = r.recognize_google(audio)
                    st.success(f"üó£Ô∏è You said: {text}")
                    return text
                except sr.UnknownValueError:
                    st.error("üòï I couldn't understand that. Try again slowly.")
                except sr.RequestError:
                    st.error("‚ö†Ô∏è Speech service unavailable. Please use text input.")
            return None
        except Exception as e:
            # Catch PyAudio/PortAudio related errors that happen when sr.Microphone() is called
            st.error(f"‚ö†Ô∏è Voice input is not supported in this deployment environment. Please use Text or Image input.")
            # st.code(f"Error: {e}", language="text") # Optional: show the error for debugging
            return None
            
except Exception:
    # If the system dependency (PyAudio) is missing during import, this block executes.
    # VOICE_AVAILABLE remains False.
    
    # Define a placeholder function for the UI to call without crashing
    def listen_to_voice():
        st.error("‚ö†Ô∏è Voice input requires system libraries (PyAudio) that are not installed on this server. Please use Text or Image input.")
        return None

# -----------------------------
# üîç Smarter Identification
# -----------------------------
def identify_butterfly(description):
    if not description:
        return None

    desc = description.lower().strip()
    desc = re.sub(r"[^\w\s]", " ", desc)

    keywords = {
        "Monarch": ["orange", "black", "veins", "striped", "north america"],
        "Swallowtail": ["yellow", "black", "tail", "swallow", "wingtip"],
        "Blue Morpho": ["blue", "shiny", "metallic", "iridescent", "morpho"],
        "Painted Lady": ["painted", "lady", "brown", "orange", "spotted"],
        "Common Jezebel": ["white", "red", "yellow", "jezebel", "tree top"],
        "Peacock": ["brown", "eye", "eyespot", "peacock", "circle"],
        "Red Admiral": ["red", "admiral", "black", "white band", "bold"],
    }

    scores = {}
    for species, words in keywords.items():
        scores[species] = sum(word in desc for word in words)

    best_species = max(scores, key=scores.get)
    if scores[best_species] == 0:
        best_species = random.choice(list(keywords.keys()))
    return best_species

# -----------------------------
# ü¶ã Interface
# -----------------------------
mode = st.radio("Choose input method:", ["Describe", "Voice", "Upload Image"])

if "history" not in st.session_state:
    st.session_state.history = []
if "identified" not in st.session_state:
    st.session_state.identified = False

species = None

if mode == "Describe":
    text = st.text_input("Describe your butterfly (e.g. 'orange with black veins'):")
    if st.button("Identify"):
        species = identify_butterfly(text)

elif mode == "Voice":
    st.markdown("### üó£Ô∏è Voice Input (via Browser Microphone)")
    st.info("Click START, speak your butterfly description for a few seconds, then click STOP.")
    
    # 1. Start the WebRTC streamer
    webrtc_ctx = webrtc_streamer(
        key="speech-rec",
        mode=WebRtcMode.SENDONLY,
        audio_processor_factory=AudioRecorder,
        media_stream_constraints={"video": False, "audio": True},
    )

    # Check if the session is ready and the processor instance is available
    if webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
        processor: AudioRecorder = webrtc_ctx.audio_processor

        # 2. Control the recording process
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üî¥ START Recording", use_container_width=True):
                processor.is_recording = True
                processor.audio_chunks = [] # Clear previous recording
                st.session_state.voice_status = "Recording..."
            
        with col2:
            if st.button("‚èπÔ∏è STOP & Identify", use_container_width=True):
                processor.is_recording = False
                st.session_state.voice_status = "Processing..."
        
        st.caption(st.session_state.get("voice_status", "Ready to record."))


        # 3. Process the audio when recording stops
        if not processor.is_recording and processor.audio_chunks:
            # Reconstruct the audio data for the speech_recognition library
            
            # The component gives us numpy arrays, we convert them to bytes
            import numpy as np
            audio_data_np = np.concatenate(processor.audio_chunks, axis=0)
            audio_bytes = audio_data_np.tobytes()
            
            # Create a dummy AudioData object for the recognizer
            r = sr.Recognizer()
            
            # IMPORTANT: This step requires knowing the sample rate/width of the audio.
            # Assuming 16000 sample rate and 2 bytes (16-bit) sample width
            # You might need to adjust these based on your environment!
            audio_data = sr.AudioData(audio_bytes, 16000, 2)
            
            st.session_state.voice_status = "Sending to Google..."
            st.rerun() # Rerun to update status while waiting
            
            try:
                # Use Google Speech Recognition to get text
                text = r.recognize_google(audio_data)
                st.success(f"üó£Ô∏è You said: {text}")
                
                # Identify the butterfly using the transcribed text
                species = identify_butterfly(text)

            except sr.UnknownValueError:
                st.error("üòï I couldn't understand that. Try again slowly.")
            except sr.RequestError as e:
                st.error(f"‚ö†Ô∏è Speech service unavailable. Error: {e}")
            
            # Clear chunks after processing
            processor.audio_chunks = []

elif mode == "Upload Image":
    file = st.file_uploader("Upload image", type=["jpg", "png", "jpeg"])
    if file and st.button("Identify Uploaded Image"):
        filename = file.name.lower()
        species = identify_butterfly(filename)

if species:
    st.success(f"Species Identified: **{species}** ü¶ã")
    st.image(butterflies[species], caption=species, use_container_width=True)
    st.info(facts.get(species, "Lovely butterfly!"))
    st.session_state.history.append(species)
    st.session_state.identified = True

# -----------------------------
# üåø Recently Identified Gallery
# -----------------------------
if st.session_state.history:
    st.markdown("### üïäÔ∏è Recently Identified Butterflies")
    cols = st.columns(3)
    recent = st.session_state.history[-6:][::-1]
    for i, past_species in enumerate(recent):
        with cols[i % 3]:
            st.image(butterflies[past_species], caption=past_species, use_container_width=True)

# -----------------------------
# üåç Migration Tracker
# -----------------------------
if st.session_state.identified:
    st.markdown("---")
    st.subheader("üß≠ Migration Tracker")

    selected_species = st.selectbox("Select Butterfly:", list(migration_data.keys()))
    months = [d["month"] for d in migration_data[selected_species]]
    selected_month = st.select_slider("Select Month", options=months)

    df = pd.DataFrame(migration_data[selected_species])
    current = next(d for d in migration_data[selected_species] if d["month"] == selected_month)

    scatter = pdk.Layer(
        "ScatterplotLayer",
        df,
        get_position=["lon", "lat"],
        get_color=[46, 139, 87, 200],
        get_radius=300000,
        pickable=True,
    )
    path = pdk.Layer(
        "PathLayer",
        data=[{"path": [[r["lon"], r["lat"]] for r in migration_data[selected_species]]}],
        get_path="path",
        get_width=4,
        get_color=[0, 200, 0],
    )
    tooltip = {"text": "{place}\n{reason}\nüí° {fact}"}
    view = pdk.ViewState(latitude=current["lat"], longitude=current["lon"], zoom=3)
    deck = pdk.Deck(layers=[path, scatter], initial_view_state=view, map_style=None, tooltip=tooltip)
    st.pydeck_chart(deck, use_container_width=True)

    st.markdown(
        f"""
        <div style='background-color:#f0fff0;padding:12px;border-radius:10px;border-left:6px solid #2E8B57;margin-top:10px;'>
        <b>üìç Location:</b> {current['place']}<br>
        <b>üóìÔ∏è Month:</b> {current['month']}<br>
        <b>üí° Fun Fact:</b> {current['fact']}
        </div>
        """,
        unsafe_allow_html=True
    )

# -----------------------------
# üéØ Quiz Section
# -----------------------------
st.markdown("---")
st.subheader("üéØ Butterfly Quiz")

if "quiz_species" not in st.session_state:
    st.session_state.quiz_species = random.choice(list(butterflies.keys()))

st.image(butterflies[st.session_state.quiz_species], caption="Guess the species ü¶ã", use_container_width=True)
guess = st.text_input("Your Guess:")
if st.button("Check Answer"):
    if guess.lower().strip() == st.session_state.quiz_species.lower():
        st.success("üéâ Correct! You know your butterflies!")
        st.balloons()
    else:
        st.error(f"‚ùå Oops! It was **{st.session_state.quiz_species}**.")
    st.session_state.quiz_species = random.choice(list(butterflies.keys()))

# -----------------------------
# üå∏ Footer
# -----------------------------
st.markdown("<hr>", unsafe_allow_html=True)
st.markdown("<p style='text-align:center; color:#2E8B57;'>Made with ‚ù§Ô∏è and curiosity ü¶ã</p>", unsafe_allow_html=True)
